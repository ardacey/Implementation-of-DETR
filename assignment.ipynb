{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tiny-DETR for PennFudanPed (Assignment 4)\n",
        "Lightweight DETR implementation faithful to the paper (Carion et al., 2020) with MobileNetV2 and ResNet18 backbones. The notebook is Colab-friendly and includes data prep, model definition, Hungarian matching loss, training, evaluation (mAP@0.5), and visualization hooks. All explanations are in English; code cells have short helper comments for clarity.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d10be3d4",
      "metadata": {},
      "source": [
        "## How to run in Colab\n",
        "- Runtime → Change runtime type → GPU (A100/T4/V100 all work).\n",
        "- Execute cells in order. The download cell grabs PennFudanPed and reorganizes it into `PennFudanPed/Pedestrian/{train,val,test}` with images + annotations together.\n",
        "- Default epochs are small for smoke tests. Bump to ~20-30 for the actual assignment runs (baseline + at least two guided improvements).\n",
        "- Save checkpoints to Drive if you want persistence.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "b60377d6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch version: 2.9.1\n",
            "CUDA available: False\n",
            "Python: 3.13.7\n",
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "import math, random, json, re, copy, time, shutil, torch, platform\n",
        "from pathlib import Path\n",
        "import urllib.request, zipfile\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision\n",
        "from torchvision.transforms import functional as TF\n",
        "from torchvision.transforms import ColorJitter\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def set_seed(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(123)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "print('PyTorch version:', torch.__version__)\n",
        "print('CUDA available:', torch.cuda.is_available())\n",
        "print('Python:', platform.python_version())\n",
        "print('Using device:', device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dea2b86f",
      "metadata": {},
      "source": [
        "## Dataset (organized folders)\n",
        "PennFudanPed has 170 pedestrian images. We place data in `PennFudanPed/Pedestrian/{train,val,test}`, each containing paired `.png` and `.txt` files. If the organized folders already exist, we just reuse them; otherwise we download and reorganize using the official split lists baked into the notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02ad30f9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Organized data root: PennFudanPed/Pedestrian\n"
          ]
        }
      ],
      "source": [
        "DATA_ROOT = Path('PennFudanPed')\n",
        "ORG_ROOT = DATA_ROOT / 'Pedestrian'\n",
        "\n",
        "organized_root = ORG_ROOT\n",
        "print('Organized data root:', organized_root)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83f4ca6d",
      "metadata": {},
      "source": [
        "## Data pipeline\n",
        "- Files are read from `PennFudanPed/Pedestrian/<split>` (PNG + TXT side by side).\n",
        "- Fixed 512×512 resolution (requirement). No padding is used because images are resized.\n",
        "- Two augmentations: random horizontal flip and mild color jitter; both motivated by the paper's suggestion for basic robustness on small datasets.\n",
        "- Targets are converted to normalized center-format boxes (`cx, cy, w, h` in [0,1]) as in DETR.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "91ed7706",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset sizes: 118 25 27\n"
          ]
        }
      ],
      "source": [
        "def parse_annotation(txt_path: Path):\n",
        "    \"\"\"Extract pedestrian boxes from a PennFudanPed annotation file.\"\"\"\n",
        "    lines = txt_path.read_text().splitlines()\n",
        "    boxes = []\n",
        "    for ln in lines:\n",
        "        if 'Bounding box for object' in ln:\n",
        "            coords = re.findall(r'\\((\\d+), (\\d+)\\)', ln)\n",
        "            if len(coords) == 2:\n",
        "                (x0, y0), (x1, y1) = coords\n",
        "                boxes.append([float(x0), float(y0), float(x1), float(y1)])\n",
        "    return boxes\n",
        "\n",
        "def xyxy_to_cxcywh(boxes: torch.Tensor):\n",
        "    x0, y0, x1, y1 = boxes.unbind(-1)\n",
        "    return torch.stack([(x0 + x1) / 2, (y0 + y1) / 2, (x1 - x0), (y1 - y0)], dim=-1)\n",
        "\n",
        "def cxcywh_to_xyxy(boxes: torch.Tensor):\n",
        "    cx, cy, w, h = boxes.unbind(-1)\n",
        "    x0 = cx - 0.5 * w\n",
        "    y0 = cy - 0.5 * h\n",
        "    x1 = cx + 0.5 * w\n",
        "    y1 = cy + 0.5 * h\n",
        "    return torch.stack([x0, y0, x1, y1], dim=-1)\n",
        "\n",
        "class Compose:\n",
        "    def __init__(self, transforms):\n",
        "        self.transforms = transforms\n",
        "    def __call__(self, image, target):\n",
        "        for t in self.transforms:\n",
        "            image, target = t(image, target)\n",
        "        return image, target\n",
        "\n",
        "class RandomHorizontalFlip:\n",
        "    def __init__(self, p=0.5):\n",
        "        self.p = p\n",
        "    def __call__(self, image, target):\n",
        "        if random.random() < self.p:\n",
        "            image = TF.hflip(image)\n",
        "            w, _ = image.size\n",
        "            if target is not None and len(target.get('boxes', [])):\n",
        "                boxes = target['boxes'].clone()\n",
        "                boxes[:, [0, 2]] = w - boxes[:, [2, 0]]\n",
        "                target['boxes'] = boxes\n",
        "        return image, target\n",
        "\n",
        "class RandomColor:\n",
        "    def __init__(self, brightness=0.2, contrast=0.2, saturation=0.2, hue=0.02):\n",
        "        self.op = ColorJitter(brightness=brightness, contrast=contrast, saturation=saturation, hue=hue)\n",
        "    def __call__(self, image, target):\n",
        "        return self.op(image), target\n",
        "\n",
        "class ResizeToSquare:\n",
        "    def __init__(self, size=512):\n",
        "        self.size = size\n",
        "    def __call__(self, image, target):\n",
        "        w, h = image.size\n",
        "        image = TF.resize(image, (self.size, self.size))\n",
        "        if target is not None and len(target.get('boxes', [])):\n",
        "            scale_x, scale_y = self.size / w, self.size / h\n",
        "            boxes = target['boxes'] * torch.tensor([scale_x, scale_y, scale_x, scale_y])\n",
        "            target['boxes'] = boxes\n",
        "        if target is not None:\n",
        "            target['size'] = torch.tensor([self.size, self.size], dtype=torch.int64)\n",
        "        return image, target\n",
        "\n",
        "class ToTensorAndNormalize:\n",
        "    def __init__(self, mean=None, std=None):\n",
        "        self.mean = mean or [0.485, 0.456, 0.406]\n",
        "        self.std = std or [0.229, 0.224, 0.225]\n",
        "    def __call__(self, image, target):\n",
        "        image = TF.to_tensor(image)\n",
        "        image = TF.normalize(image, self.mean, self.std)\n",
        "        return image, target\n",
        "\n",
        "class ConvertToDetrTargets:\n",
        "    def __call__(self, image, target):\n",
        "        if target is None:\n",
        "            return image, target\n",
        "        boxes = target.get('boxes', torch.zeros((0, 4)))\n",
        "        if boxes.numel() > 0:\n",
        "            boxes = xyxy_to_cxcywh(boxes)\n",
        "            w, h = target['size'][1].item(), target['size'][0].item()\n",
        "            scale = torch.tensor([w, h, w, h], dtype=torch.float32)\n",
        "            boxes = boxes / scale\n",
        "        target['boxes'] = boxes\n",
        "        target['area'] = (boxes[:, 2] * boxes[:, 3]) if boxes.numel() > 0 else torch.zeros((0,), dtype=torch.float32)\n",
        "        target['iscrowd'] = torch.zeros((boxes.shape[0],), dtype=torch.int64)\n",
        "        return image, target\n",
        "\n",
        "def make_transforms(train: bool = True, image_size: int = 512):\n",
        "    ops = []\n",
        "    if train:\n",
        "        ops.append(RandomColor())\n",
        "        ops.append(RandomHorizontalFlip(p=0.5))\n",
        "    ops.append(ResizeToSquare(size=image_size))\n",
        "    ops.append(ConvertToDetrTargets())\n",
        "    ops.append(ToTensorAndNormalize())\n",
        "    return Compose(ops)\n",
        "\n",
        "def list_split_files(split_dir: Path):\n",
        "    return sorted([p for p in split_dir.glob('*.png')])\n",
        "\n",
        "class PennFudanDataset(Dataset):\n",
        "    def __init__(self, split_dir: Path, transforms=None, image_size: int = 512):\n",
        "        self.split_dir = Path(split_dir)\n",
        "        self.files = list_split_files(self.split_dir)\n",
        "        self.transforms = transforms\n",
        "        self.image_size = image_size\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.files[idx]\n",
        "        ann_path = img_path.with_suffix('.txt')\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        orig_w, orig_h = image.size\n",
        "        boxes = parse_annotation(ann_path)\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32).reshape(-1, 4)\n",
        "        labels = torch.zeros((boxes.shape[0],), dtype=torch.int64)  # single class index 0\n",
        "        target = {\n",
        "            'boxes': boxes,\n",
        "            'labels': labels,\n",
        "            'image_id': torch.tensor(idx),\n",
        "            'orig_size': torch.tensor([orig_h, orig_w])\n",
        "        }\n",
        "        if self.transforms is not None:\n",
        "            image, target = self.transforms(image, target)\n",
        "        return image, target\n",
        "\n",
        "def collate_fn(batch):\n",
        "    images, targets = list(zip(*batch))\n",
        "    images = torch.stack(images)\n",
        "    return images, list(targets)\n",
        "\n",
        "image_size = 512\n",
        "train_ds = PennFudanDataset(organized_root / 'train', transforms=make_transforms(train=True, image_size=image_size), image_size=image_size)\n",
        "val_ds = PennFudanDataset(organized_root / 'val', transforms=make_transforms(train=False, image_size=image_size), image_size=image_size)\n",
        "test_ds = PennFudanDataset(organized_root / 'test', transforms=make_transforms(train=False, image_size=image_size), image_size=image_size)\n",
        "\n",
        "print('Dataset sizes:', len(train_ds), len(val_ds), len(test_ds))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9640d80",
      "metadata": {},
      "source": [
        "## Model components (Tiny-DETR)\n",
        "Key ideas from the paper retained in the lightweight version:\n",
        "- CNN backbone (MobileNetV2 or ResNet18) to produce a feature map.\n",
        "- 2D sinusoidal positional encoding added to the flattened feature map.\n",
        "- Transformer encoder-decoder with learned object queries (set prediction, fixed query count).\n",
        "- Shared FFN heads for class logits (+ no-object) and normalized box regression.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26004799",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Box utilities\n",
        "\n",
        "def box_area(boxes):\n",
        "    return (boxes[:, 2] - boxes[:, 0]).clamp(min=0) * (boxes[:, 3] - boxes[:, 1]).clamp(min=0)\n",
        "\n",
        "def box_iou(boxes1, boxes2):\n",
        "    area1 = box_area(boxes1)\n",
        "    area2 = box_area(boxes2)\n",
        "    lt = torch.max(boxes1[:, None, :2], boxes2[:, :2])\n",
        "    rb = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])\n",
        "    wh = (rb - lt).clamp(min=0)\n",
        "    inter = wh[:, :, 0] * wh[:, :, 1]\n",
        "    union = area1[:, None] + area2 - inter\n",
        "    iou = inter / (union + 1e-6)\n",
        "    return iou, union\n",
        "\n",
        "def generalized_box_iou(boxes1, boxes2):\n",
        "    iou, union = box_iou(boxes1, boxes2)\n",
        "    lt = torch.min(boxes1[:, None, :2], boxes2[:, :2])\n",
        "    rb = torch.max(boxes1[:, None, 2:], boxes2[:, 2:])\n",
        "    wh = (rb - lt).clamp(min=0)\n",
        "    area = wh[:, :, 0] * wh[:, :, 1]\n",
        "    return iou - (area - union) / (area + 1e-6)\n",
        "\n",
        "class PositionEncodingSine(nn.Module):\n",
        "    def __init__(self, num_pos_feats=128, temperature=10000, normalize=True, scale=None):\n",
        "        super().__init__()\n",
        "        self.num_pos_feats = num_pos_feats\n",
        "        self.temperature = temperature\n",
        "        self.normalize = normalize\n",
        "        self.scale = scale or 2 * math.pi\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        # x: [B, C, H, W]\n",
        "        b, c, h, w = x.shape\n",
        "        mask = torch.zeros((b, h, w), device=x.device, dtype=torch.bool)\n",
        "        y_embed = mask.cumsum(1, dtype=torch.float32)\n",
        "        x_embed = mask.cumsum(2, dtype=torch.float32)\n",
        "        if self.normalize:\n",
        "            eps = 1e-6\n",
        "            y_embed = (y_embed + 0.5) / (y_embed[:, -1:, :] + eps) * self.scale\n",
        "            x_embed = (x_embed + 0.5) / (x_embed[:, :, -1:] + eps) * self.scale\n",
        "        dim_t = self.temperature ** (2 * (torch.arange(self.num_pos_feats, device=x.device) // 2) / self.num_pos_feats)\n",
        "        pos_x = x_embed[:, :, :, None] / dim_t\n",
        "        pos_y = y_embed[:, :, :, None] / dim_t\n",
        "        pos_x = torch.stack((pos_x[:, :, :, 0::2].sin(), pos_x[:, :, :, 1::2].cos()), dim=4).flatten(3)\n",
        "        pos_y = torch.stack((pos_y[:, :, :, 0::2].sin(), pos_y[:, :, :, 1::2].cos()), dim=4).flatten(3)\n",
        "        pos = torch.cat((pos_y, pos_x), dim=3).permute(0, 3, 1, 2)\n",
        "        return pos\n",
        "\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation='relu'):\n",
        "        super().__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n",
        "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.activation = F.relu if activation == 'relu' else F.gelu\n",
        "    def with_pos_embed(self, tensor, pos):\n",
        "        return tensor if pos is None else tensor + pos\n",
        "    def forward(self, src, src_mask=None, src_key_padding_mask=None, pos=None):\n",
        "        q = k = self.with_pos_embed(src, pos)\n",
        "        src2 = self.self_attn(q, k, value=src, attn_mask=src_mask, key_padding_mask=src_key_padding_mask)[0]\n",
        "        src = self.norm1(src + self.dropout1(src2))\n",
        "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
        "        src = self.norm2(src + self.dropout2(src2))\n",
        "        return src\n",
        "\n",
        "class TransformerDecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation='relu'):\n",
        "        super().__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n",
        "        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n",
        "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "        self.activation = F.relu if activation == 'relu' else F.gelu\n",
        "    def with_pos_embed(self, tensor, pos):\n",
        "        return tensor if pos is None else tensor + pos\n",
        "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None, tgt_key_padding_mask=None,\n",
        "                memory_key_padding_mask=None, pos=None, query_pos=None):\n",
        "        q = k = self.with_pos_embed(tgt, query_pos)\n",
        "        tgt2 = self.self_attn(q, k, value=tgt, attn_mask=tgt_mask, key_padding_mask=tgt_key_padding_mask)[0]\n",
        "        tgt = self.norm1(tgt + self.dropout1(tgt2))\n",
        "        tgt2 = self.multihead_attn(query=self.with_pos_embed(tgt, query_pos),\n",
        "                                   key=self.with_pos_embed(memory, pos),\n",
        "                                   value=memory, attn_mask=memory_mask,\n",
        "                                   key_padding_mask=memory_key_padding_mask)[0]\n",
        "        tgt = self.norm2(tgt + self.dropout2(tgt2))\n",
        "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n",
        "        tgt = self.norm3(tgt + self.dropout3(tgt2))\n",
        "        return tgt\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, encoder_layer, num_layers, norm=None):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([copy.deepcopy(encoder_layer) for _ in range(num_layers)])\n",
        "        self.norm = norm\n",
        "    def forward(self, src, mask=None, src_key_padding_mask=None, pos=None):\n",
        "        output = src\n",
        "        for layer in self.layers:\n",
        "            output = layer(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask, pos=pos)\n",
        "        if self.norm is not None:\n",
        "            output = self.norm(output)\n",
        "        return output\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, decoder_layer, num_layers, norm=None, return_intermediate=False):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([copy.deepcopy(decoder_layer) for _ in range(num_layers)])\n",
        "        self.norm = norm\n",
        "        self.return_intermediate = return_intermediate\n",
        "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None, tgt_key_padding_mask=None,\n",
        "                memory_key_padding_mask=None, pos=None, query_pos=None):\n",
        "        output = tgt\n",
        "        intermediate = []\n",
        "        for layer in self.layers:\n",
        "            output = layer(output, memory, tgt_mask=tgt_mask, memory_mask=memory_mask,\n",
        "                           tgt_key_padding_mask=tgt_key_padding_mask, memory_key_padding_mask=memory_key_padding_mask,\n",
        "                           pos=pos, query_pos=query_pos)\n",
        "            if self.return_intermediate:\n",
        "                intermediate.append(self.norm(output) if self.norm is not None else output)\n",
        "        if self.norm is not None:\n",
        "            output = self.norm(output)\n",
        "            if self.return_intermediate:\n",
        "                intermediate[-1] = output\n",
        "        if self.return_intermediate:\n",
        "            return torch.stack(intermediate)\n",
        "        return output.unsqueeze(0)\n",
        "\n",
        "class SimpleTransformer(nn.Module):\n",
        "    def __init__(self, d_model=256, nhead=8, num_encoder_layers=3, num_decoder_layers=3,\n",
        "                 dim_feedforward=1024, dropout=0.1, return_intermediate_dec=True):\n",
        "        super().__init__()\n",
        "        encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout)\n",
        "        decoder_layer = TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout)\n",
        "        self.encoder = TransformerEncoder(encoder_layer, num_encoder_layers, norm=nn.LayerNorm(d_model))\n",
        "        self.decoder = TransformerDecoder(decoder_layer, num_decoder_layers, norm=nn.LayerNorm(d_model),\n",
        "                                          return_intermediate=return_intermediate_dec)\n",
        "        self.d_model = d_model\n",
        "    def forward(self, src, mask, query_embed, pos_embed):\n",
        "        bs, c, h, w = src.shape\n",
        "        src_flat = src.flatten(2).permute(0, 2, 1)\n",
        "        pos_flat = pos_embed.flatten(2).permute(0, 2, 1)\n",
        "        query_embed = query_embed.unsqueeze(0).expand(bs, -1, -1)\n",
        "        tgt = torch.zeros_like(query_embed)\n",
        "        memory = self.encoder(src_flat, mask=None, src_key_padding_mask=None, pos=pos_flat)\n",
        "        hs = self.decoder(tgt, memory, memory_key_padding_mask=None, pos=pos_flat, query_pos=query_embed)\n",
        "        return hs, memory\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        for i in range(num_layers):\n",
        "            in_dim = input_dim if i == 0 else hidden_dim\n",
        "            out_dim = output_dim if i == num_layers - 1 else hidden_dim\n",
        "            layers.append(nn.Linear(in_dim, out_dim))\n",
        "        self.layers = nn.ModuleList(layers)\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers[:-1]:\n",
        "            x = F.relu(layer(x))\n",
        "        return self.layers[-1](x)\n",
        "\n",
        "def build_backbone(name='resnet18', train_backbone=False):\n",
        "    name = name.lower()\n",
        "    if name == 'resnet18':\n",
        "        weights = torchvision.models.ResNet18_Weights.IMAGENET1K_V1\n",
        "        backbone = torchvision.models.resnet18(weights=weights)\n",
        "        modules = list(backbone.children())[:-2]\n",
        "        backbone = nn.Sequential(*modules)\n",
        "        num_channels = 512\n",
        "    elif name in ['mobilenet_v2', 'mobilenetv2']:\n",
        "        weights = torchvision.models.MobileNet_V2_Weights.IMAGENET1K_V2\n",
        "        backbone = torchvision.models.mobilenet_v2(weights=weights).features\n",
        "        num_channels = 1280\n",
        "    else:\n",
        "        raise ValueError(f'Unknown backbone {name}')\n",
        "    for param in backbone.parameters():\n",
        "        param.requires_grad = train_backbone\n",
        "    return backbone, num_channels\n",
        "\n",
        "class TinyDETR(nn.Module):\n",
        "    def __init__(self, backbone_name='resnet18', num_classes=1, num_queries=50, hidden_dim=256,\n",
        "                 nheads=8, num_encoder_layers=3, num_decoder_layers=3, dim_feedforward=1024,\n",
        "                 dropout=0.1, aux_loss=True, train_backbone=False):\n",
        "        super().__init__()\n",
        "        self.backbone, num_backbone_channels = build_backbone(backbone_name, train_backbone=train_backbone)\n",
        "        self.input_proj = nn.Conv2d(num_backbone_channels, hidden_dim, kernel_size=1)\n",
        "        self.position_embedding = PositionEncodingSine(hidden_dim // 2, normalize=True)\n",
        "        self.query_embed = nn.Embedding(num_queries, hidden_dim)\n",
        "        self.transformer = SimpleTransformer(d_model=hidden_dim, nhead=nheads,\n",
        "                                             num_encoder_layers=num_encoder_layers,\n",
        "                                             num_decoder_layers=num_decoder_layers,\n",
        "                                             dim_feedforward=dim_feedforward,\n",
        "                                             dropout=dropout, return_intermediate_dec=aux_loss)\n",
        "        self.class_embed = nn.Linear(hidden_dim, num_classes + 1)  # +1 for no-object\n",
        "        self.bbox_embed = MLP(hidden_dim, hidden_dim, 4, num_layers=3)\n",
        "        self.aux_loss = aux_loss\n",
        "    def forward(self, samples):\n",
        "        if isinstance(samples, (list, tuple)):\n",
        "            samples = torch.stack(samples)\n",
        "        features = self.backbone(samples)\n",
        "        src = self.input_proj(features)\n",
        "        pos = self.position_embedding(src)\n",
        "        hs, memory = self.transformer(src, mask=None, query_embed=self.query_embed.weight, pos_embed=pos)\n",
        "        outputs_class = self.class_embed(hs)\n",
        "        outputs_coord = self.bbox_embed(hs).sigmoid()\n",
        "        out = {'pred_logits': outputs_class[-1], 'pred_boxes': outputs_coord[-1]}\n",
        "        if self.aux_loss:\n",
        "            out['aux_outputs'] = [{'pred_logits': c, 'pred_boxes': b} for c, b in zip(outputs_class[:-1], outputs_coord[:-1])]\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9fb4b07c",
      "metadata": {},
      "source": [
        "## Hungarian matching + loss\n",
        "- One-to-one assignment between predictions and ground truth with Hungarian matching (set prediction objective).\n",
        "- Loss = classification (cross-entropy with no-object), L1 box loss, and GIoU loss. Weights mirror DETR’s recipe but can be tuned.\n",
        "- Auxiliary decoder outputs receive the same loss to aid optimization on the shallow transformer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7cccf88f",
      "metadata": {},
      "outputs": [],
      "source": [
        "class HungarianMatcher(nn.Module):\n",
        "    def __init__(self, class_cost=1.0, bbox_cost=5.0, giou_cost=2.0):\n",
        "        super().__init__()\n",
        "        self.class_cost = class_cost\n",
        "        self.bbox_cost = bbox_cost\n",
        "        self.giou_cost = giou_cost\n",
        "        assert class_cost != 0 or bbox_cost != 0 or giou_cost != 0, 'All costs are zero!'\n",
        "    @torch.no_grad()\n",
        "    def forward(self, outputs, targets):\n",
        "        bs, num_queries = outputs['pred_logits'].shape[:2]\n",
        "        out_prob = outputs['pred_logits'].softmax(-1)  # [B, Q, K]\n",
        "        out_bbox = outputs['pred_boxes']\n",
        "        indices = []\n",
        "        for b in range(bs):\n",
        "            tgt_ids = targets[b]['labels']\n",
        "            tgt_bbox = targets[b]['boxes']\n",
        "            if tgt_bbox.numel() == 0:\n",
        "                indices.append((torch.as_tensor([], dtype=torch.int64), torch.as_tensor([], dtype=torch.int64)))\n",
        "                continue\n",
        "            prob = out_prob[b][:, tgt_ids]\n",
        "            cost_class = -prob\n",
        "            cost_bbox = torch.cdist(out_bbox[b], tgt_bbox, p=1)\n",
        "            cost_giou = -generalized_box_iou(cxcywh_to_xyxy(out_bbox[b]), cxcywh_to_xyxy(tgt_bbox))\n",
        "            C = self.class_cost * cost_class + self.bbox_cost * cost_bbox + self.giou_cost * cost_giou\n",
        "            C = C.cpu()\n",
        "            from scipy.optimize import linear_sum_assignment\n",
        "            i, j = linear_sum_assignment(C)\n",
        "            indices.append((torch.as_tensor(i, dtype=torch.int64), torch.as_tensor(j, dtype=torch.int64)))\n",
        "        return indices\n",
        "\n",
        "def get_src_permutation_idx(indices):\n",
        "    batch_idx = torch.cat([torch.full_like(src, i) for i, (src, _) in enumerate(indices)])\n",
        "    src_idx = torch.cat([src for (src, _) in indices])\n",
        "    return batch_idx, src_idx\n",
        "\n",
        "class SetCriterion(nn.Module):\n",
        "    def __init__(self, num_classes, matcher, weight_dict, eos_coef, losses):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.matcher = matcher\n",
        "        self.weight_dict = weight_dict\n",
        "        self.eos_coef = eos_coef\n",
        "        self.losses = losses\n",
        "        empty_weight = torch.ones(self.num_classes + 1)\n",
        "        empty_weight[-1] = self.eos_coef\n",
        "        self.register_buffer('empty_weight', empty_weight)\n",
        "    def loss_labels(self, outputs, targets, indices):\n",
        "        src_logits = outputs['pred_logits']\n",
        "        idx = get_src_permutation_idx(indices)\n",
        "        target_classes_o = torch.cat([t['labels'][J] for t, (_, J) in zip(targets, indices)], dim=0)\n",
        "        target_classes = torch.full(src_logits.shape[:2], self.num_classes, dtype=torch.int64, device=src_logits.device)\n",
        "        target_classes[idx] = target_classes_o\n",
        "        loss_ce = F.cross_entropy(src_logits.transpose(1, 2), target_classes, self.empty_weight)\n",
        "        return {'loss_ce': loss_ce}\n",
        "    def loss_boxes(self, outputs, targets, indices):\n",
        "        idx = get_src_permutation_idx(indices)\n",
        "        src_boxes = outputs['pred_boxes'][idx]\n",
        "        target_boxes = torch.cat([t['boxes'][i] for t, (_, i) in zip(targets, indices)], dim=0)\n",
        "        loss_bbox = F.l1_loss(src_boxes, target_boxes, reduction='none')\n",
        "        losses = {'loss_bbox': loss_bbox.mean()}\n",
        "        loss_giou = 1 - torch.diag(generalized_box_iou(cxcywh_to_xyxy(src_boxes), cxcywh_to_xyxy(target_boxes)))\n",
        "        losses['loss_giou'] = loss_giou.mean()\n",
        "        return losses\n",
        "    def _get_loss(self, loss, outputs, targets, indices):\n",
        "        loss_map = {'labels': self.loss_labels, 'boxes': self.loss_boxes}\n",
        "        return loss_map[loss](outputs, targets, indices)\n",
        "    def forward(self, outputs, targets):\n",
        "        outputs_no_aux = {k: v for k, v in outputs.items() if k != 'aux_outputs'}\n",
        "        indices = self.matcher(outputs_no_aux, targets)\n",
        "        losses = {}\n",
        "        for loss in self.losses:\n",
        "            losses.update(self._get_loss(loss, outputs_no_aux, targets, indices))\n",
        "        if 'aux_outputs' in outputs:\n",
        "            for i, aux in enumerate(outputs['aux_outputs']):\n",
        "                idxs = self.matcher(aux, targets)\n",
        "                for loss in self.losses:\n",
        "                    l_dict = self._get_loss(loss, aux, targets, idxs)\n",
        "                    l_dict = {f'{k}_{i}': v for k, v in l_dict.items()}\n",
        "                    losses.update(l_dict)\n",
        "        total = sum(self.weight_dict.get(k, 1.0) * v for k, v in losses.items())\n",
        "        losses['loss_total'] = total\n",
        "        return losses\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "683593b1",
      "metadata": {},
      "source": [
        "## Training + evaluation utilities\n",
        "- AdamW optimizer with a smaller LR for the backbone (paper suggestion).\n",
        "- Optional gradient clipping and AMP for speed on Colab.\n",
        "- Simple mAP@0.5 implementation for the single-class setup.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "849738ea",
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_model(config):\n",
        "    model = TinyDETR(\n",
        "        backbone_name=config['backbone'],\n",
        "        num_classes=1,\n",
        "        num_queries=config['num_queries'],\n",
        "        hidden_dim=config['hidden_dim'],\n",
        "        nheads=config['nheads'],\n",
        "        num_encoder_layers=config['enc_layers'],\n",
        "        num_decoder_layers=config['dec_layers'],\n",
        "        dim_feedforward=config['ff_dim'],\n",
        "        dropout=config['dropout'],\n",
        "        aux_loss=True,\n",
        "        train_backbone=config['train_backbone']\n",
        "    )\n",
        "    matcher = HungarianMatcher(class_cost=config['cls_cost'], bbox_cost=config['bbox_cost'], giou_cost=config['giou_cost'])\n",
        "    weight_dict = {'loss_ce': config['cls_loss_coef'], 'loss_bbox': config['bbox_loss_coef'], 'loss_giou': config['giou_loss_coef']}\n",
        "    criterion = SetCriterion(num_classes=1, matcher=matcher, weight_dict=weight_dict, eos_coef=config['no_object_weight'], losses=['labels', 'boxes'])\n",
        "    return model, criterion\n",
        "\n",
        "def build_loaders(config):\n",
        "    train_loader = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True,\n",
        "                              collate_fn=collate_fn, num_workers=config['num_workers'])\n",
        "    val_loader = DataLoader(val_ds, batch_size=config['batch_size'], shuffle=False,\n",
        "                            collate_fn=collate_fn, num_workers=config['num_workers'])\n",
        "    test_loader = DataLoader(test_ds, batch_size=config['batch_size'], shuffle=False,\n",
        "                             collate_fn=collate_fn, num_workers=config['num_workers'])\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "def targets_to_device(targets, device):\n",
        "    new_targets = []\n",
        "    for t in targets:\n",
        "        new_t = {k: v.to(device) if torch.is_tensor(v) else v for k, v in t.items()}\n",
        "        new_targets.append(new_t)\n",
        "    return new_targets\n",
        "\n",
        "def train_one_epoch(model, criterion, data_loader, optimizer, device, epoch, max_norm=0.1, scaler=None):\n",
        "    model.train()\n",
        "    criterion.train()\n",
        "    losses_all = []\n",
        "    ce_all, bbox_all, giou_all = [], [], []\n",
        "    for images, targets in data_loader:\n",
        "        images = images.to(device)\n",
        "        targets = targets_to_device(targets, device)\n",
        "        optimizer.zero_grad()\n",
        "        with torch.cuda.amp.autocast(enabled=scaler is not None):\n",
        "            outputs = model(images)\n",
        "            loss_dict = criterion(outputs, targets)\n",
        "            loss = loss_dict['loss_total']\n",
        "        if scaler is not None:\n",
        "            scaler.scale(loss).backward()\n",
        "            if max_norm > 0:\n",
        "                scaler.unscale_(optimizer)\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            loss.backward()\n",
        "            if max_norm > 0:\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
        "            optimizer.step()\n",
        "        losses_all.append(loss.item())\n",
        "        ce_all.append(loss_dict['loss_ce'].item())\n",
        "        bbox_all.append(loss_dict['loss_bbox'].item())\n",
        "        giou_all.append(loss_dict['loss_giou'].item())\n",
        "    stats = {\n",
        "        'loss': float(np.mean(losses_all)),\n",
        "        'loss_ce': float(np.mean(ce_all)),\n",
        "        'loss_bbox': float(np.mean(bbox_all)),\n",
        "        'loss_giou': float(np.mean(giou_all))\n",
        "    }\n",
        "    print(f\"Epoch {epoch}: train loss {stats['loss']:.4f} | ce {stats['loss_ce']:.4f} | bbox {stats['loss_bbox']:.4f} | giou {stats['loss_giou']:.4f}\")\n",
        "    return stats\n",
        "\n",
        "def compute_ap(recall, precision):\n",
        "    recall = np.concatenate(([0.0], recall, [1.0]))\n",
        "    precision = np.concatenate(([0.0], precision, [0.0]))\n",
        "    for i in range(precision.size - 1, 0, -1):\n",
        "        precision[i - 1] = np.maximum(precision[i - 1], precision[i])\n",
        "    indices = np.where(recall[1:] != recall[:-1])[0]\n",
        "    ap = np.sum((recall[indices + 1] - recall[indices]) * precision[indices + 1])\n",
        "    return ap\n",
        "\n",
        "def evaluate_map(model, data_loader, device, score_thr=0.3, iou_thr=0.5):\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    gts = {}\n",
        "    with torch.no_grad():\n",
        "        for images, targets in data_loader:\n",
        "            images = images.to(device)\n",
        "            targets = targets_to_device(targets, device)\n",
        "            outputs = model(images)\n",
        "            probs = outputs['pred_logits'].softmax(-1)\n",
        "            boxes = outputs['pred_boxes']\n",
        "            for b in range(images.shape[0]):\n",
        "                tgt = targets[b]\n",
        "                img_id = int(tgt['image_id'])\n",
        "                h, w = tgt['size']\n",
        "                gt_boxes = cxcywh_to_xyxy(tgt['boxes']) * torch.tensor([w, h, w, h], device=device)\n",
        "                gts[img_id] = gt_boxes.cpu()\n",
        "                ped_scores = probs[b, :, 0]\n",
        "                ped_boxes = cxcywh_to_xyxy(boxes[b]) * torch.tensor([w, h, w, h], device=device)\n",
        "                keep = ped_scores > score_thr\n",
        "                for score, box in zip(ped_scores[keep], ped_boxes[keep]):\n",
        "                    preds.append({'image_id': img_id, 'score': float(score.cpu()), 'box': box.cpu()})\n",
        "    preds = sorted(preds, key=lambda x: x['score'], reverse=True)\n",
        "    tp, fp = [], []\n",
        "    matched = {img_id: np.zeros(len(gts[img_id])) for img_id in gts}\n",
        "    for pred in preds:\n",
        "        img_id = pred['image_id']\n",
        "        box = pred['box']\n",
        "        if img_id not in gts or len(gts[img_id]) == 0:\n",
        "            fp.append(1); tp.append(0); continue\n",
        "        ious, _ = box_iou(box.unsqueeze(0), gts[img_id])\n",
        "        ious = ious.squeeze(0).numpy()\n",
        "        best_idx = np.argmax(ious)\n",
        "        if ious[best_idx] >= iou_thr and matched[img_id][best_idx] == 0:\n",
        "            tp.append(1); fp.append(0); matched[img_id][best_idx] = 1\n",
        "        else:\n",
        "            fp.append(1); tp.append(0)\n",
        "    tp = np.array(tp)\n",
        "    fp = np.array(fp)\n",
        "    if len(tp) == 0:\n",
        "        return {'map50': 0.0, 'precision': 0.0, 'recall': 0.0}\n",
        "    tp_cum = np.cumsum(tp)\n",
        "    fp_cum = np.cumsum(fp)\n",
        "    recalls = tp_cum / (sum(len(v) for v in gts.values()) + 1e-6)\n",
        "    precisions = tp_cum / np.maximum(tp_cum + fp_cum, 1e-6)\n",
        "    ap = compute_ap(recalls, precisions)\n",
        "    return {'map50': float(ap), 'precision': float(precisions[-1]), 'recall': float(recalls[-1])}\n",
        "\n",
        "def run_training(config, train_loader, val_loader):\n",
        "    model, criterion = build_model(config)\n",
        "    model.to(device)\n",
        "    criterion.to(device)\n",
        "    backbone_params = []\n",
        "    transformer_params = []\n",
        "    for name, p in model.named_parameters():\n",
        "        if not p.requires_grad:\n",
        "            continue\n",
        "        if 'backbone' in name:\n",
        "            backbone_params.append(p)\n",
        "        else:\n",
        "            transformer_params.append(p)\n",
        "    param_dicts = [{'params': transformer_params}]\n",
        "    if backbone_params:\n",
        "        param_dicts.append({'params': backbone_params, 'lr': config['lr_backbone']})\n",
        "    optimizer = torch.optim.AdamW(param_dicts, lr=config['lr'], weight_decay=config['weight_decay'])\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=config['lr_drop'], gamma=0.1)\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=config['use_amp'])\n",
        "    history = []\n",
        "    best_map = -1\n",
        "    save_path = Path(config['checkpoint_path'])\n",
        "    save_path.parent.mkdir(exist_ok=True, parents=True)\n",
        "    for epoch in range(1, config['epochs'] + 1):\n",
        "        train_stats = train_one_epoch(model, criterion, train_loader, optimizer, device, epoch,\n",
        "                                      max_norm=config['clip_max_norm'], scaler=scaler)\n",
        "        val_stats = evaluate_map(model, val_loader, device, score_thr=config['score_threshold'])\n",
        "        scheduler.step()\n",
        "        record = {'epoch': epoch, **train_stats, **val_stats, 'lr': optimizer.param_groups[0]['lr']}\n",
        "        history.append(record)\n",
        "        print(f\"Val mAP@0.5: {val_stats['map50']:.3f} | precision {val_stats['precision']:.3f} | recall {val_stats['recall']:.3f}\")\n",
        "        if val_stats['map50'] > best_map:\n",
        "            best_map = val_stats['map50']\n",
        "            torch.save({'model': model.state_dict(), 'config': config, 'epoch': epoch}, save_path)\n",
        "            print(f'✅ New best model saved to {save_path} (mAP@0.5={best_map:.3f})')\n",
        "    return model, history\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f10b2104",
      "metadata": {},
      "source": [
        "## Baseline configuration (ResNet18)\n",
        "Paper-inspired defaults, shrunk for the small dataset. Increase epochs to ~25 for the full run.\n",
        "- Backbone: ResNet18 (ImageNet-pretrained), LR 1e-5.\n",
        "- Transformer: 3 encoder / 3 decoder layers, 8 heads, hidden dim 256, FFN dim 1024.\n",
        "- Queries: 50 (tunable).\n",
        "- Loss weights: cls=1, bbox=5, giou=2, no-object weight=0.1.\n",
        "- Augmentations: color jitter + horizontal flip.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27a78b0a",
      "metadata": {},
      "outputs": [],
      "source": [
        "base_config = {\n",
        "    'backbone': 'resnet18',\n",
        "    'num_queries': 50,\n",
        "    'hidden_dim': 256,\n",
        "    'nheads': 8,\n",
        "    'enc_layers': 3,\n",
        "    'dec_layers': 3,\n",
        "    'ff_dim': 1024,\n",
        "    'dropout': 0.1,\n",
        "    'train_backbone': False,\n",
        "    'lr': 1e-4,\n",
        "    'lr_backbone': 1e-5,\n",
        "    'weight_decay': 1e-4,\n",
        "    'lr_drop': 15,\n",
        "    'batch_size': 4,\n",
        "    'num_workers': 2,\n",
        "    'epochs': 10,  # set to 20-30 for the full assignment run\n",
        "    'clip_max_norm': 0.1,\n",
        "    'score_threshold': 0.3,\n",
        "    'cls_cost': 1.0,\n",
        "    'bbox_cost': 5.0,\n",
        "    'giou_cost': 2.0,\n",
        "    'cls_loss_coef': 1.0,\n",
        "    'bbox_loss_coef': 5.0,\n",
        "    'giou_loss_coef': 2.0,\n",
        "    'no_object_weight': 0.1,\n",
        "    'use_amp': True,\n",
        "    'checkpoint_path': 'checkpoints/tiny_detr_resnet18.pth'\n",
        "}\n",
        "train_loader, val_loader, test_loader = build_loaders(base_config)\n",
        "print('Ready loaders. Baseline config:')\n",
        "print(json.dumps({k: v for k, v in base_config.items() if k not in ['checkpoint_path']}, indent=2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ce7be55",
      "metadata": {},
      "source": [
        "### Train baseline\n",
        "Set `base_config['epochs']=20` (or 25) for the graded run. Keep the cell idempotent so you can re-run with new hyperparameters. Training time on Colab T4 for 20 epochs is typically ~20–30 minutes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "173fc481",
      "metadata": {},
      "outputs": [],
      "source": [
        "RUN_BASELINE = False  # flip to True to launch training\n",
        "if RUN_BASELINE:\n",
        "    model, history = run_training(base_config, train_loader, val_loader)\n",
        "    with open('history_resnet18.json', 'w') as f:\n",
        "        json.dump(history, f, indent=2)\n",
        "    print('Training complete. History saved to history_resnet18.json')\n",
        "else:\n",
        "    print('Skipping baseline training (set RUN_BASELINE=True to run).')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "898c4def",
      "metadata": {},
      "source": [
        "## Guided improvements\n",
        "Try at least two changes (paper-guided): different backbone, query count, transformer depth, or augmentation tweaks. Two ready-made configs are below; feel free to add more.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fce44e62",
      "metadata": {},
      "outputs": [],
      "source": [
        "mobile_config = {**base_config, 'backbone': 'mobilenet_v2', 'lr_backbone': 5e-5,\n",
        "                 'hidden_dim': 192, 'num_queries': 75, 'checkpoint_path': 'checkpoints/tiny_detr_mobilenetv2.pth'}\n",
        "queries_config = {**base_config, 'num_queries': 100, 'checkpoint_path': 'checkpoints/tiny_detr_resnet18_q100.pth'}\n",
        "\n",
        "print('Example configs prepared (mobilenet_v2 and resnet18 with more queries). Set RUN_EXPERIMENT to train them.')\n",
        "RUN_EXPERIMENT = False\n",
        "if RUN_EXPERIMENT:\n",
        "    for cfg in [mobile_config, queries_config]:\n",
        "        print(f\"Running config: {cfg['checkpoint_path']}\")\n",
        "        model, history = run_training(cfg, *build_loaders(cfg))\n",
        "        with open(Path(cfg['checkpoint_path']).with_suffix('.history.json'), 'w') as f:\n",
        "            json.dump(history, f, indent=2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load best checkpoint and evaluate on test set\n",
        "After training, load the saved weights and compute test mAP@0.5. This cell is safe to run multiple times with different checkpoints.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_model_from_checkpoint(path, config):\n",
        "    ckpt = torch.load(path, map_location=device)\n",
        "    cfg = ckpt.get('config', config)\n",
        "    model, _ = build_model(cfg)\n",
        "    model.load_state_dict(ckpt['model'])\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "CHECKPOINT_TO_EVAL = base_config['checkpoint_path']\n",
        "if Path(CHECKPOINT_TO_EVAL).exists():\n",
        "    eval_model = load_model_from_checkpoint(CHECKPOINT_TO_EVAL, base_config)\n",
        "    test_stats = evaluate_map(eval_model, test_loader, device, score_thr=base_config['score_threshold'])\n",
        "    print('Test mAP@0.5:', test_stats)\n",
        "else:\n",
        "    print('No checkpoint found at', CHECKPOINT_TO_EVAL)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualization (qualitative results)\n",
        "Use this to inspect predictions. Good + failure cases should be included in the report.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def denormalize(img_tensor):\n",
        "    mean = torch.tensor([0.485, 0.456, 0.406], device=img_tensor.device).view(3, 1, 1)\n",
        "    std = torch.tensor([0.229, 0.224, 0.225], device=img_tensor.device).view(3, 1, 1)\n",
        "    return (img_tensor * std + mean).clamp(0, 1)\n",
        "\n",
        "def visualize_predictions(model, dataset, num_images=3, score_thr=0.4):\n",
        "    model.eval()\n",
        "    idxs = random.sample(range(len(dataset)), k=num_images)\n",
        "    fig, axes = plt.subplots(1, num_images, figsize=(6 * num_images, 6))\n",
        "    if num_images == 1:\n",
        "        axes = [axes]\n",
        "    with torch.no_grad():\n",
        "        for ax, idx in zip(axes, idxs):\n",
        "            img, target = dataset[idx]\n",
        "            h, w = target['size']\n",
        "            inp = img.unsqueeze(0).to(device)\n",
        "            out = model(inp)\n",
        "            prob = out['pred_logits'].softmax(-1)[0, :, 0]\n",
        "            boxes = cxcywh_to_xyxy(out['pred_boxes'][0]) * torch.tensor([w, h, w, h], device=device)\n",
        "            keep = prob > score_thr\n",
        "            img_np = denormalize(img).permute(1, 2, 0).cpu().numpy()\n",
        "            ax.imshow(img_np)\n",
        "            for score, box in zip(prob[keep], boxes[keep]):\n",
        "                x0, y0, x1, y1 = box.cpu()\n",
        "                rect = plt.Rectangle((x0, y0), x1 - x0, y1 - y0, fill=False, color='lime', linewidth=2)\n",
        "                ax.add_patch(rect)\n",
        "                ax.text(x0, y0, f'{score:.2f}', color='yellow', fontsize=9, bbox=dict(facecolor='black', alpha=0.5))\n",
        "            gt_boxes = cxcywh_to_xyxy(target['boxes']) * torch.tensor([w, h, w, h])\n",
        "            for box in gt_boxes:\n",
        "                x0, y0, x1, y1 = box\n",
        "                rect = plt.Rectangle((x0, y0), x1 - x0, y1 - y0, fill=False, color='red', linestyle='--', linewidth=1.5)\n",
        "                ax.add_patch(rect)\n",
        "            ax.set_title(f'Image {idx} | green=pred, red=gt')\n",
        "            ax.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Example usage (requires a trained checkpoint loaded into eval_model)\n",
        "# visualize_predictions(eval_model, val_ds, num_images=3, score_thr=0.5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Report template (fill after experiments)\n",
        "- **Backbone comparison (MobileNetV2 vs ResNet18):** describe feature strength vs. speed, validation/test mAP@0.5, training time/epoch. Relate to DETR paper notes on backbone capacity.\n",
        "- **Positional encoding:** summarize why DETR needs it and confirm sinusoidal implementation.\n",
        "- **Transformer depth / queries:** report query counts tried (e.g., 50 vs 100) and impact on convergence.\n",
        "- **Augmentation study:** effect of color jitter + flip (and any additional tweak) on mAP or overfitting.\n",
        "- **Final model:** chosen config, epochs, LR schedule, best val mAP, test mAP, and qualitative examples (good + failure cases).\n",
        "- **Training curves:** include loss + val mAP plots (load from history JSON files).\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
